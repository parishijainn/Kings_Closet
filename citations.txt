Below the dashed line, include easily understandable and verifiable 
citations to all the major sources you used for your project, as described
in the TP document: 
https://www.cs.cmu.edu/~112/notes/term-project-and-hack112.html#tp-policies

In addition, your code must also include citations directly in the code that 
make it clear where you use code that is partly or entirely not of your 
original design, and what the source is for that code.
------------------------------------>

Andrew ID: parishij
I initially wrote the logic of the analyzeColors function to analyze colors in 
clothing images using OpenCV and KMeans clustering. My implementation extracted 
RGB pixel values, clustered them to find dominant colors, and attempted to name 
them based on threshold ranges I defined for common fashion colors.

To learn how to implement K-Means Clustering, I used the following articles:
- "Image Color Segmentation by K-Means Clustering Algorithm" (Medium)
  https://medium.com/data-science/image-color-segmentation-by-k-means-
  clustering-algorithm-5792e563f26e

- "K-Means Clustering in OpenCV and Application for Color Quantization"
  (Machine Learning Mastery)
  https://www.machinelearningmastery.com/k-means-clustering-in-opencv-and-
  application-for-color-quantization/

- "Deconstructing an Image with Pixels" (Medium)
  https://medium.com/@ys3372/deconstructing-an-image-with-pixels-4c65c3a2268c

After implementing the basics, I used ChatGPT to help me improve and debug it. 
My prompt to GPT was:

    "im making an outfit grading game that analyzes clothes using image colors. 
    i wrote this that gets the main colors with KMeans and maps them to 
    named thresholds like 'blue' or 'red.' im having issues with mismatches 
    because a color might look correct but returns unknown. can you help me
    with this function so it better handles unknown colors and gives fallback 
    results if it cant find an exact match?"

GPT helped me refine the color matching logic, especially by:
- Improving how color clusters are sorted by dominance
- Introducing a fallback Euclidean distance match when threshold matching fails
- Suggesting a structure to cache color results to avoid recomputation

This function plays a key role in grading outfits based on color compatibility 
rules I defined, making sure top and bottom clothing items visually match.

For the handtracking code, it was originally developed for an old assignment.
I worked on the original handtracking implementation and am 
reusing and expanding upon my earlier code in this project. 
This version builds on that foundation by integrating outfit randomization
(via 5-finger detection) and app-specific interactions like swipe detection
for clothing changes.



andrewID: pnihalan
-- Virtual Try On --
-  I used this video to help me figure out how to 
navigate openCV, https://www.youtube.com/watch?v=6C7R24-Ze10
this video was my main source of help for the writing of the virtual try
on function
I also used this video to teach me how to use handtracking and mediapipe
https://www.youtube.com/watch?v=RRBXVu5UE-U

- Swipe-cooldown logic : I used AI help to discover how to 
leverage Python’s time module to enforce a cooldown 
between each swipe event in the try-on loop. 
I asked it : "How can i make my handtracking less laggy and make it not 
jump picture to picture.
what if gave me: 
def overlayPng(frame, overlayImage, positionX, positionY):
    overlayHeight, overlayWidth = overlayImage.shape[:2]
    frameHeight, frameWidth = frame.shape[:2]

    if positionX + overlayWidth < 0 or positionX > frameWidth or \
       positionY + overlayHeight < 0 or positionY > frameHeight:
        return frame


- Alpha blending implementation: I used AI guidance to figure
 out how to blend the RGBA overlay PNG into
  the BGR camera frame—specifically how 
  to extract and apply the alpha channel 
  for smooth transparency
i asked it: When I draw the PNG on top of the video, 
the see-through parts either disappear or turn opaque, 
and the edges look jagged. How can I write code so that 
each pixel of the PNG mixes smoothly
what it gave me: 
def overlayPng(frame, overlayImage, positionX, positionY):
    overlayHeight, overlayWidth = overlayImage.shape[:2]
    frameHeight, frameWidth = frame.shape[:2]

    if positionX + overlayWidth < 0 or positionX > frameWidth or \
       positionY + overlayHeight < 0 or positionY > frameHeight:
        return frame

- Landmark midpoint calculation: 
I used AI to derive the formula for computing 
the midpoint between two pose landmarks 
(shoulders, hips) so that the 
overlays would center correctly on the body.
i asked it: “What’s the formula to compute the midpoint
 between two MediaPipe pose landmarks (in pixel coords) 
 so I can center overlays on shoulders or hips?”
what it gave: 
shoulderX = int((leftShoulder.x + 
                             rightShoulder.x) / 2 * frameWidth)
            shoulderY = int((leftShoulder.y + 
                             rightShoulder.y) / 2 * frameHeight)
            hipX = int((leftHip.x + rightHip.x) / 2 * frameWidth)
            hipY = int((leftHip.y + rightHip.y) / 2 * frameHeight)









-lsponhou:

for the function def putImgagesIntoLists(app), I asked chatGpt the prompt ,
 "if I have png files in a folder in my vs code, is there a way to loop through 
 all the images in the folder to add them to a list or do I need to do it 
 manually?," to which it responded, 
 "Yes — if you already have .png files in a folder (like images/), you can 
 automatically loop through them and add them to a list, no need to manually 
 list each one!

Here’s how to do it in Python:

import os

def loadImagesFromFolder(folderPath):
    imageList = []
    for filename in os.listdir(folderPath):
        if filename.lower().endswith('.png'):
            fullPath = os.path.join(folderPath, filename)
            imageList.append(fullPath)
    return imageList

# Example usage:
app.tops = loadImagesFromFolder('images/tops')
app.bottoms = loadImagesFromFolder('images/bottoms')
